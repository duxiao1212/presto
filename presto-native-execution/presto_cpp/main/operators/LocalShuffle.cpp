/*
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#include "presto_cpp/main/operators/LocalShuffle.h"
#include "presto_cpp/external/json/nlohmann/json.hpp"
#include "presto_cpp/main/common/Configs.h"

#include "velox/common/file/FileInputStream.h"

#include <folly/lang/Bits.h>
#include <boost/range/algorithm/sort.hpp>


namespace facebook::presto::operators {

using json = nlohmann::json;

namespace {

using TStreamIdx = uint16_t;

/// SortedFileInputStream reads sorted (key, data) pairs from a single
/// shuffle file with buffered I/O. It extends FileInputStream for efficient
/// buffered I/O and implements MergeStream interface for k-way merge.
class SortedFileInputStream final : public velox::common::FileInputStream,
                                    public velox::MergeStream {
 public:
  SortedFileInputStream(
      const std::string& filePath,
      TStreamIdx streamIdx,
      velox::memory::MemoryPool* pool,
      size_t bufferSize = kDefaultInputStreamBufferSize)
      : velox::common::FileInputStream(
            velox::filesystems::getFileSystem(filePath, nullptr)
                ->openFileForRead(filePath),
            bufferSize,
            pool),
        streamIdx_(streamIdx) {
    // Initialize by reading first row.
    next();
  }

  ~SortedFileInputStream() override = default;

  /// Advances to next entry. Returns false if EOF reached.
  bool next() {
    if (atEnd()) {
      currentKey_ = std::string_view();
      currentData_ = std::string_view();
      return false;
    }
    const TRowSize keySize = folly::Endian::big(read<TRowSize>());
    const TRowSize dataSize = folly::Endian::big(read<TRowSize>());

    currentKey_ = nextStringView(keySize, keyStorage_);
    currentData_ = nextStringView(dataSize, dataStorage_);

    return true;
  }

  std::string_view currentKey() const {
    return currentKey_;
  }

  std::string_view currentData() const {
    return currentData_;
  }

  bool hasData() const override {
    return !currentData_.empty() || !atEnd();
  }

  bool operator<(const velox::MergeStream& other) const override {
    const auto* otherReader = static_cast<const SortedFileInputStream*>(&other);
    const auto cmp = compareKeys(currentKey_, otherReader->currentKey_);
    if (cmp != std::strong_ordering::equal) {
      return cmp == std::strong_ordering::less;
    }
    // Tie-break using stream index for deterministic ordering
    return streamIdx_ < otherReader->streamIdx_;
  }

 private:
 /// Returns a string view of the next 'size' bytes using nextView
  std::string_view nextStringView(TRowSize size, std::string& storage) {
    if (size == 0) {
      return {};
    }
    const auto view = nextView(size);
    if (view.size() == size) {
      return view;
    }

    // Data crosses buffer boundary - must copy
    storage.resize(size);
    std::memcpy(storage.data(), view.data(), view.size());
    readBytes(
        reinterpret_cast<uint8_t*>(storage.data()) + view.size(),
        size - view.size());
    return std::string_view(storage);
  }

  const TStreamIdx streamIdx_;
  // Views into the FileInputStream buffer.
  std::string_view currentKey_;
  std::string_view currentData_;

  // Temporary storage for edge cases when data crosses buffer boundaries
  std::string keyStorage_;
  std::string dataStorage_;
};

std::vector<RowMetadata>
extractRowMetadata(const char* buffer, size_t bufferSize, bool sortedShuffle) {
  std::vector<RowMetadata> rows;
  size_t offset = 0;

  if (sortedShuffle) {
    // Format: keySize | dataSize | key | data
    while (offset + sizeof(TRowSize) * 2 <= bufferSize) {
      const size_t rowStart = offset;

      const TRowSize keySize = folly::Endian::big(
          *reinterpret_cast<const TRowSize*>(buffer + offset));
      offset += sizeof(TRowSize);

      const TRowSize dataSize = folly::Endian::big(
          *reinterpret_cast<const TRowSize*>(buffer + offset));
      offset += sizeof(TRowSize);

      VELOX_CHECK_LE(
          offset + keySize + dataSize,
          bufferSize,
          "Corrupted shuffle data: expected {} bytes for row (offset={}, keySize={}, dataSize={}) but only {} bytes available in buffer",
          offset + keySize + dataSize,
          offset,
          keySize,
          dataSize,
          bufferSize);

      rows.push_back(
          RowMetadata{
              .rowStart = rowStart, .keySize = keySize, .dataSize = dataSize});

      offset += keySize + dataSize;
    }
  } else {
    // Format: dataSize | data
    while (offset + sizeof(TRowSize) <= bufferSize) {
      const size_t rowStart = offset;

      const TRowSize dataSize = folly::Endian::big(
          *reinterpret_cast<const TRowSize*>(buffer + offset));
      offset += sizeof(TRowSize);

      VELOX_CHECK_LE(
          offset + dataSize,
          bufferSize,
          "Corrupted shuffle data: expected {} bytes for row (offset={}, dataSize={}) but only {} bytes available in buffer",
          offset + dataSize,
          offset,
          dataSize,
          bufferSize);

      rows.push_back(
          RowMetadata{
              .rowStart = rowStart, .keySize = 0, .dataSize = dataSize});

      offset += dataSize;
    }
  }

  return rows;
}

inline std::string_view
extractRowData(const RowMetadata& row, const char* buffer, bool sortedShuffle) {
  const auto dataOffset = row.rowStart + (sortedShuffle ? (kUint32Size * 2) + row.keySize : kUint32Size);
  return {buffer + dataOffset, row.dataSize};
}

std::vector<RowMetadata> extractAndSortRowMetadata(
    const char* buffer,
    size_t bufferSize,
    bool sortedShuffle) {
  auto rows = extractRowMetadata(buffer, bufferSize, sortedShuffle);
  if (!rows.empty() && sortedShuffle) {
    boost::range::sort(
        rows,
        [buffer](const RowMetadata& lhs, const RowMetadata& rhs) {
          const char* lhsKey = buffer + lhs.rowStart + (kUint32Size * 2);
          const char* rhsKey = buffer + rhs.rowStart + (kUint32Size * 2);
          return compareKeys(
                     std::string_view(lhsKey, lhs.keySize),
                     std::string_view(rhsKey, rhs.keySize)) ==
              std::strong_ordering::less;
        });
  }
  return rows;
}

inline std::string createShuffleFileName(
    const std::string& rootPath,
    const std::string& queryId,
    uint32_t shuffleId,
    int32_t partition,
    int fileIndex,
    const std::thread::id& id) {
  // Follow Spark's shuffle file name format: shuffle_shuffleId_0_reduceId
  return fmt::format(
      "{}/{}_shuffle_{}_0_{}_{}_{}.bin",
      rootPath,
      queryId,
      shuffleId,
      partition,
      fileIndex,
      id);
}
} // namespace

LocalShuffleWriteInfo LocalShuffleWriteInfo::deserialize(
    const std::string& info) {
  const auto jsonReadInfo = json::parse(info);
  LocalShuffleWriteInfo shuffleInfo;
  jsonReadInfo.at("rootPath").get_to(shuffleInfo.rootPath);
  jsonReadInfo.at("queryId").get_to(shuffleInfo.queryId);
  jsonReadInfo.at("shuffleId").get_to(shuffleInfo.shuffleId);
  jsonReadInfo.at("numPartitions").get_to(shuffleInfo.numPartitions);
  return shuffleInfo;
}

LocalShuffleReadInfo LocalShuffleReadInfo::deserialize(
    const std::string& info) {
  const auto jsonReadInfo = json::parse(info);
  LocalShuffleReadInfo shuffleInfo;
  jsonReadInfo.at("rootPath").get_to(shuffleInfo.rootPath);
  jsonReadInfo.at("queryId").get_to(shuffleInfo.queryId);
  jsonReadInfo.at("partitionIds").get_to(shuffleInfo.partitionIds);
  return shuffleInfo;
}

LocalShuffleWriter::LocalShuffleWriter(
    const std::string& rootPath,
    const std::string& queryId,
    uint32_t shuffleId,
    uint32_t numPartitions,
    uint64_t maxBytesPerPartition,
    bool sortedShuffle,
    velox::memory::MemoryPool* pool)
    : threadId_(std::this_thread::get_id()),
      pool_(pool),
      numPartitions_(numPartitions),
      maxBytesPerPartition_(maxBytesPerPartition),
      sortedShuffle_(sortedShuffle),
      rootPath_(rootPath),
      queryId_(queryId),
      shuffleId_(shuffleId) {
  inProgressPartitions_.assign(numPartitions_, nullptr);
  inProgressSizes_.assign(numPartitions_, 0);
  fileSystem_ = velox::filesystems::getFileSystem(rootPath_, nullptr);
}

void LocalShuffleWriter::writeBlock(int32_t partition) {
  auto& buffer = inProgressPartitions_[partition];
  const auto bufferSize = inProgressSizes_[partition];

  VELOX_DCHECK_NOT_NULL(buffer, "Buffer should be allocated before writeBlock");
  VELOX_DCHECK_GT(bufferSize, 0, "Buffer size should be positive");

  auto file = getNextOutputFile(partition);
  const char* data = buffer->as<char>();

  // For non-sorted shuffle, write buffer directly
  if (!sortedShuffle_) {
    file->append(std::string_view(data, bufferSize));
  } else {
    // For sorted shuffle, parse and sort rows, then write
    const auto sortedRows =
        extractAndSortRowMetadata(data, bufferSize, sortedShuffle_);
    for (const auto& row : sortedRows) {
      const size_t rowLen = sortedShuffle_
          ? (kUint32Size * 2) + row.keySize + row.dataSize
          : kUint32Size + row.dataSize;
      file->append(std::string_view(data + row.rowStart, rowLen));
    }
  }
  file->close();
  inProgressSizes_[partition] = 0;
}

std::unique_ptr<velox::WriteFile> LocalShuffleWriter::getNextOutputFile(
    int32_t partition) {
  auto filename = nextAvailablePartitionFileName(rootPath_, partition);
  return fileSystem_->openFileForWrite(filename);
}

std::string LocalShuffleWriter::nextAvailablePartitionFileName(
    const std::string& root,
    int32_t partition) const {
  int fileCount = 0;
  std::string filename;
  // TODO: consider to maintain the next to create file count in memory as we
  // always do cleanup when switch to a new root directory path.
  do {
    filename = createShuffleFileName(
        root, queryId_, shuffleId_, partition, fileCount, threadId_);
    if (!fileSystem_->exists(filename)) {
      break;
    }
    ++fileCount;
  } while (true);

  return filename;
}

size_t LocalShuffleWriter::rowSize(size_t keySize, size_t dataSize) const {
  return sortedShuffle_ ? (kUint32Size * 2) + keySize + dataSize
                        : kUint32Size + dataSize;
}

void LocalShuffleWriter::appendRow(
    char* writePos,
    std::string_view key,
    std::string_view data) {
  if (sortedShuffle_) {
    const auto keySize = static_cast<TRowSize>(key.size());
    const auto dataSize = static_cast<TRowSize>(data.size());
    *reinterpret_cast<TRowSize*>(writePos) = folly::Endian::big(keySize);
    writePos += sizeof(TRowSize);
    *reinterpret_cast<TRowSize*>(writePos) = folly::Endian::big(dataSize);
    writePos += sizeof(TRowSize);
    if (keySize > 0) {
      memcpy(writePos, key.data(), keySize);
      writePos += keySize;
    }
    if (dataSize > 0) {
      memcpy(writePos, data.data(), dataSize);
    }
  } else {
    const auto dataSize = static_cast<TRowSize>(data.size());
    *reinterpret_cast<TRowSize*>(writePos) = folly::Endian::big(dataSize);
    writePos += sizeof(TRowSize);
    if (dataSize > 0) {
      memcpy(writePos, data.data(), dataSize);
    }
  }
}

void LocalShuffleWriter::collect(
    int32_t partition,
    std::string_view key,
    std::string_view data) {
  VELOX_CHECK_LT(partition, numPartitions_);
  VELOX_CHECK(
      sortedShuffle_ || key.empty(),
      "key '{}' must be empty for non-sorted shuffle",
      key);
  const auto rowSize = this->rowSize(key.size(), data.size());

  auto& buffer = inProgressPartitions_[partition];
  if (buffer == nullptr) {
    buffer = velox::AlignedBuffer::allocate<char>(
        std::max(static_cast<uint64_t>(rowSize), maxBytesPerPartition_),
        pool_,
        0);
    inProgressSizes_[partition] = 0;
  } else if (inProgressSizes_[partition] + rowSize >= buffer->capacity()) {
    writeBlock(partition);
  }
  auto* rawBuffer = buffer->asMutable<char>();
  auto* writePos = rawBuffer + inProgressSizes_[partition];
  appendRow(writePos, key, data);
  inProgressSizes_[partition] += rowSize;
}

void LocalShuffleWriter::noMoreData(bool success) {
  // Delete all shuffle files on failure.
  if (!success) {
    cleanup();
  }
  for (auto i = 0; i < numPartitions_; ++i) {
    if (inProgressSizes_[i] > 0) {
      writeBlock(i);
    }
  }
}

LocalShuffleReader::LocalShuffleReader(
    const std::string& rootPath,
    const std::string& queryId,
    std::vector<std::string> partitionIds,
    bool sortedShuffle,
    velox::memory::MemoryPool* pool)
    : rootPath_(rootPath),
      queryId_(queryId),
      partitionIds_(std::move(partitionIds)),
      sortedShuffle_(sortedShuffle),
      pool_(pool) {
  fileSystem_ = velox::filesystems::getFileSystem(rootPath_, nullptr);
}

void LocalShuffleReader::initialize() {
  VELOX_CHECK(!initialized_, "LocalShuffleReader already initialized");

  readPartitionFiles_ = getReadPartitionFiles();

  if (sortedShuffle_ && !readPartitionFiles_.empty()) {
    std::vector<std::unique_ptr<velox::MergeStream>> streams;
    streams.reserve(readPartitionFiles_.size());
    TStreamIdx streamIdx = 0;
    for (const auto& filename : readPartitionFiles_) {
      VELOX_CHECK(
          !filename.empty(),
          "Invalid empty shuffle file path for query {}, partitions: [{}]",
          queryId_,
          folly::join(", ", partitionIds_));
      auto reader = std::make_unique<SortedFileInputStream>(
          filename, streamIdx, pool_);
      if (reader->hasData()) {
        streams.push_back(std::move(reader));
        ++streamIdx;
      }
    }
    if (!streams.empty()) {
      merge_ =
          std::make_unique<velox::TreeOfLosers<velox::MergeStream, uint16_t>>(
              std::move(streams));
    }
  }

  initialized_ = true;
}

std::vector<std::unique_ptr<ReadBatch>> LocalShuffleReader::nextSorted(
    uint64_t maxBytes) {
  std::vector<std::unique_ptr<ReadBatch>> batches;

  if (merge_ == nullptr) {
    return batches;
  }

  auto batchBuffer = velox::AlignedBuffer::allocate<char>(maxBytes, pool_, 0);
  std::vector<std::string_view> rows;
  uint64_t bufferUsed = 0;

  while (auto* stream = merge_->next()) {
    auto* reader = dynamic_cast<SortedFileInputStream*>(stream);
    const auto data = reader->currentData();
    const auto rowSize = kUint32Size + data.size();

    // With the current row the bufferUsed byte will exceed the maxBytes
    if (bufferUsed + rowSize > maxBytes) {
      if (bufferUsed > 0) {
        // We have some rows already, return them to release the memory
        batches.push_back(
            std::make_unique<ReadBatch>(std::move(rows), std::move(batchBuffer)));
        return batches;
      }
      // Single row exceeds buffer - allocate larger buffer for this row
      batchBuffer = velox::AlignedBuffer::allocate<char>(rowSize, pool_, 0);
      bufferUsed = 0;
    }

    // Write row: [dataSize][data]
    char* writePos = batchBuffer->asMutable<char>() + bufferUsed;
    *reinterpret_cast<TRowSize*>(writePos) =
        folly::Endian::big(static_cast<TRowSize>(data.size()));

    if (!data.empty()) {
      memcpy(writePos + sizeof(TRowSize), data.data(), data.size());
    }

    rows.emplace_back(batchBuffer->as<char>() + bufferUsed, rowSize);
    bufferUsed += rowSize;

    reader->next();
  }

  if (!rows.empty()) {
    batches.push_back(
        std::make_unique<ReadBatch>(std::move(rows), std::move(batchBuffer)));
  }

  return batches;
}

std::vector<std::unique_ptr<ReadBatch>> LocalShuffleReader::nextUnsorted(
    uint64_t maxBytes) {
  std::vector<std::unique_ptr<ReadBatch>> batches;
  uint64_t totalBytes{0};

  while (readPartitionFileIndex_ < readPartitionFiles_.size()) {
    const auto filename = readPartitionFiles_[readPartitionFileIndex_];
    auto file = fileSystem_->openFileForRead(filename);
    const auto fileSize = file->size();


    // TODO: Refactor to use streaming I/O with bounded buffer size instead of
    // loading entire files into memory at once. A streaming approach would
    // reduce peak memory consumption and enable processing arbitrarily large
    // shuffle files while maintaining constant memory usage.
    if (!batches.empty() && totalBytes + fileSize > maxBytes) {
      break;
    }

    auto buffer = velox::AlignedBuffer::allocate<char>(fileSize, pool_, 0);
    file->pread(0, fileSize, buffer->asMutable<void>());
    ++readPartitionFileIndex_;

    const char* data = buffer->as<char>();
    const auto parsedRows = extractRowMetadata(data, fileSize, sortedShuffle_);
    std::vector<std::string_view> rows;
    rows.reserve(parsedRows.size());
    for (const auto& row : parsedRows) {
      rows.push_back(extractRowData(row, data, sortedShuffle_));
    }

    totalBytes += fileSize;
    batches.push_back(
        std::make_unique<ReadBatch>(std::move(rows), std::move(buffer)));
  }

  return batches;
}

folly::SemiFuture<std::vector<std::unique_ptr<ReadBatch>>>
LocalShuffleReader::next(uint64_t maxBytes) {
  VELOX_CHECK(
      initialized_,
      "LocalShuffleReader::initialize() must be called before next()");
  return folly::makeSemiFuture(
      sortedShuffle_ ? nextSorted(maxBytes) : nextUnsorted(maxBytes));
}

void LocalShuffleReader::noMoreData(bool success) {
  // On failure, reset the index of the files to be read.
  if (!success) {
    readPartitionFileIndex_ = 0;
  }
}

std::vector<std::string> LocalShuffleReader::getReadPartitionFiles() const {
  // Get rid of excess '/' characters in the path.
  auto trimmedRootPath = rootPath_;
  while (trimmedRootPath.length() > 0 &&
         trimmedRootPath[trimmedRootPath.length() - 1] == '/') {
    trimmedRootPath.erase(trimmedRootPath.length() - 1, 1);
  }

  std::vector<std::string> partitionFiles;
  for (const auto& partitionId : partitionIds_) {
    auto prefix =
        fmt::format("{}/{}_{}_", trimmedRootPath, queryId_, partitionId);
    auto files = fileSystem_->list(fmt::format("{}/", rootPath_));
    for (const auto& file : files) {
      if (file.starts_with(prefix)) {
        partitionFiles.push_back(file);
      }
    }
  }

  return partitionFiles;
}

void LocalShuffleWriter::cleanup() {
  auto files = fileSystem_->list(rootPath_);
  for (auto& file : files) {
    fileSystem_->remove(file);
  }
}

std::shared_ptr<ShuffleReader> LocalPersistentShuffleFactory::createReader(
    const std::string& serializedStr,
    const int32_t /*partition*/,
    velox::memory::MemoryPool* pool) {
  const operators::LocalShuffleReadInfo readInfo =
      operators::LocalShuffleReadInfo::deserialize(serializedStr);
  auto reader = std::make_shared<LocalShuffleReader>(
      readInfo.rootPath,
      readInfo.queryId,
      readInfo.partitionIds,
      /*sortShuffle=*/false, // default to false for now
      pool);
  reader->initialize();
  return reader;
}

std::shared_ptr<ShuffleWriter> LocalPersistentShuffleFactory::createWriter(
    const std::string& serializedStr,
    velox::memory::MemoryPool* pool) {
  static const uint64_t maxBytesPerPartition =
      SystemConfig::instance()->localShuffleMaxPartitionBytes();
  const operators::LocalShuffleWriteInfo writeInfo =
      operators::LocalShuffleWriteInfo::deserialize(serializedStr);
  return std::make_shared<LocalShuffleWriter>(
      writeInfo.rootPath,
      writeInfo.queryId,
      writeInfo.shuffleId,
      writeInfo.numPartitions,
      maxBytesPerPartition,
      /*sortedShuffle=*/false, // default to false for now
      pool);
}

// Testing function to expose extractRowMetadata for tests.
// This will be removed after reader changes.
std::vector<RowMetadata> testingExtractRowMetadata(
    const char* buffer,
    size_t bufferSize,
    bool sortedShuffle) {
  return extractRowMetadata(buffer, bufferSize, sortedShuffle);
}

} // namespace facebook::presto::operators
